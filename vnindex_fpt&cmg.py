# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from xgboost import XGBRegressor, XGBClassifier
from lightgbm import LGBMRegressor, LGBMClassifier

# H√†m load d·ªØ li·ªáu
def load_data(file_path):
    return pd.read_csv(file_path)

# H√†m l√†m s·∫°ch d·ªØ li·ªáu
def clean_dataframe(df):
    rename_map = {
        'Ng√†y': 'Date',
        'L·∫ßn cu·ªëi': 'Closing Price',
        'M·ªü': 'Open Price',
        'Cao': 'Highest Price',
        'Th·∫•p': 'Lowest Price',
        'KL': 'Volume',
        '% Thay ƒë·ªïi': 'Price Change %'
    }
    df.rename(columns=rename_map, inplace=True)
    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')
    df.set_index('Date', inplace=True)

    def clean_cell(x):
        if isinstance(x, str):
            x = x.replace(',', '').replace('%', '').replace('‚àí', '-').strip()
            if 'K' in x:
                return float(x.replace('K', '')) * 1_000
            elif 'M' in x:
                return float(x.replace('M', '')) * 1_000_000
            elif 'B' in x:
                return float(x.replace('B', '')) * 1_000_000_000
            else:
                try:
                    return float(x)
                except:
                    return np.nan
        return x

    for col in df.columns:
        if col != 'Date':
            df[col] = df[col].apply(clean_cell)
            if 'Volume' in col:
                df[col] = df[col].fillna(0).astype('int64')
            elif col in ['Closing Price', 'Open Price', 'Highest Price', 'Lowest Price', 'Price Change %']:
                df[col] = df[col].astype('float')

    return df

# H√†m ph√¢n t√≠ch th·ªëng k√™ c∆° b·∫£n
def basic_stats(df, name):
    print(name)
    print(df.info())
    print("S·ªë gi√° tr·ªã b·ªã r·ªóng:")
    print(df.isnull().sum())
    print("Th·ªëng k√™ c√°c gi√° tr·ªã:")
    print(df.describe())
    print("C√°c gi√° tr·ªã ƒë·ªôc nh·∫•t c·ªßa t·ª´ng tr∆∞·ªùng d·ªØ li·ªáu:")
    for col in df.columns:
        print(f"{col}: {df[col].nunique()}")


# H√†m t·∫°o c√°c feature gi√°n ti·∫øp
def new_feature(market, stock, ticker):
    df = pd.DataFrame(index=market.index)
    df['Average Price (daily)'] = (market['Highest Price'] + market['Lowest Price']) / 2
    df['Intraday price (range)'] = market['Highest Price'] - market['Lowest Price']
    df['Actual price change'] = market['Closing Price_vnindex'] - market['Open Price']
    df['Intraday percentage volatility'] = (market['Highest Price'] - market['Lowest Price']) / 2
    return df
# EDA
def eda(df, ticker):
    """
    Th·ª±c hi·ªán ph√¢n t√≠ch d·ªØ li·ªáu kh√°m ph√° (EDA) cho m·ªôt DataFrame.

    Args:
        df (pd.DataFrame): D·ªØ li·ªáu c·∫ßn ph√¢n t√≠ch.
        ticker (str): T√™n c·ªï phi·∫øu ho·∫∑c d·ªØ li·ªáu ƒë·ªÉ hi·ªÉn th·ªã trong b√°o c√°o.

    Returns:
        None
    """
    print(f"üìã EDA Report for {ticker}")
    print("=" * 80)

    # T·ªïng quan
    print(f"Shape: {df.shape[0]} rows, {df.shape[1]} columns")
    print(f"Total Missing Values: {df.isnull().sum().sum()}")
    print(f"Duplicate Rows: {df.duplicated().sum()}")
    print("=" * 80)

    # T√≥m t·∫Øt ki·ªÉu d·ªØ li·ªáu
    print("üìö Data Types:")
    print(df.dtypes.value_counts())
    print("=" * 80)

    # Gi√° tr·ªã b·ªã thi·∫øu theo c·ªôt
    missing_cols = df.isnull().mean() * 100
    missing_cols = missing_cols[missing_cols > 0].sort_values(ascending=False)

    if not missing_cols.empty:
        print("üö© Columns with Missing Values (%):")
        print(missing_cols.round(2))

        # Bi·ªÉu ƒë·ªì ma tr·∫≠n gi√° tr·ªã b·ªã thi·∫øu
        import missingno as msno
        plt.figure(figsize=(10, 6))
        msno.matrix(df)
        plt.title('Missing Values Matrix')
        plt.show()

        # Bi·ªÉu ƒë·ªì t∆∞∆°ng quan gi√° tr·ªã b·ªã thi·∫øu
        plt.figure(figsize=(10, 6))
        msno.heatmap(df)
        plt.title('Missing Values Correlation')
        plt.show()
        plt.close()
    else:
        print("‚úÖ No Missing Values")
    print("=" * 80)

    # C·ªôt s·ªë v√† c·ªôt ph√¢n lo·∫°i
    num_cols = df.select_dtypes(include=np.number).columns.tolist()
    cat_cols = df.select_dtypes(include=["object", "category", "bool"]).columns.tolist()

    print(f"üî¢ Numerical Columns ({len(num_cols)}): {num_cols}")
    print(f"üî† Categorical Columns ({len(cat_cols)}): {cat_cols}")
    print("=" * 80)

    # T√≥m t·∫Øt c√°c c·ªôt s·ªë
    if num_cols:
        print("üìà Numerical Features Summary:")
        display(df[num_cols].describe().T)

        # Bi·ªÉu ƒë·ªì histogram
        df[num_cols].hist(bins=30, figsize=(16, 12), layout=(int(np.ceil(len(num_cols) / 3)), 3))
        plt.tight_layout()
        plt.suptitle('Histograms of Numerical Features', y=1.02)
        plt.show()
        plt.close()
    else:
        print("‚ö†Ô∏è No Numerical Columns Found")
    print("=" * 80)

    # T√≥m t·∫Øt c√°c c·ªôt ph√¢n lo·∫°i
    if cat_cols:
        print("üìä Categorical Features Summary:")
        for col in cat_cols:
            print(f"\nüìù Column: {col} (Unique: {df[col].nunique()})")
            print(df[col].value_counts(dropna=False).head(5))

            # Bi·ªÉu ƒë·ªì countplot
            plt.figure(figsize=(8, 4))
            sns.countplot(data=df, x=col, order=df[col].value_counts().index)
            plt.xticks(rotation=45)
            plt.title(f'Countplot: {col}')
            plt.show()
            plt.close()
    else:
        print("‚ö†Ô∏è No Categorical Columns Found")
    print("=" * 80)

    # Ph√¢n t√≠ch t∆∞∆°ng quan
    if len(num_cols) >= 2:
        print("üîó Correlation Matrix (Numerical Features):")
        corr = df[num_cols].corr()
        print(corr.round(2))

        # Bi·ªÉu ƒë·ªì heatmap t∆∞∆°ng quan
        plt.figure(figsize=(12, 10))
        sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", square=True)
        plt.title('Correlation Heatmap')
        plt.show()
        plt.close()
    else:
        print("‚ö†Ô∏è Not Enough Numerical Features for Correlation Analysis")
    print("=" * 80) 

# H√†m t·∫°o lag features
def create_lag(df, columns, n_lags=3, prefix=''):
    df = df.copy()
    df.index = pd.to_datetime(df.index)
    df.sort_index(inplace=True)

    for col in columns:
        for lag in range(1, n_lags + 1):
            df[f"{prefix}{col}_lag{lag}"] = df[col].shift(lag)

    return df

# 5. H√†m ph√¢n t√≠ch m√¥ h√¨nh
# 5.1. Random Forest Regressor
def rf_regressor_analysis(df, ticker, test_size=0.3, n_estimators=100, random_state=42):
    X = df.drop(columns='Closing Price')
    y = df['Closing Price']
    split_index = int(len(df) * (1 - test_size))
    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(f"MAE : {mean_absolute_error(y_test, y_pred):.4f}")
    print(f"MSE : {mean_squared_error(y_test, y_pred):.4f}")
    print(f"R¬≤  : {r2_score(y_test, y_pred):.4f}")

    importances = model.feature_importances_
    feat_imp = pd.Series(importances, index=X_train.columns).sort_values(ascending=False)
    plt.figure(figsize=(10, 6))
    sns.barplot(x=feat_imp.values, y=feat_imp.index)
    plt.title(f"Feature Importances (Regressor) of {ticker}")
    plt.show()

# 5.2. Random Forest Classifier
def rf_classifier_analysis(df, ticker, test_size=0.3, n_estimators=100, random_state=42):
    df['Target'] = (df['Closing Price'].shift(-1) > df['Closing Price']).astype(int)
    df.dropna(inplace=True)
    X = df.drop(columns=['Closing Price', 'Target'])
    y = df['Target']
    split_index = int(len(df) * (1 - test_size))
    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(f"Accuracy  : {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precision : {precision_score(y_test, y_pred):.4f}")
    print(f"Recall    : {recall_score(y_test, y_pred):.4f}")
    print(f"F1 Score  : {f1_score(y_test, y_pred):.4f}")

    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"Confusion Matrix - {ticker}")
    plt.show()

# 6. h√†m XGBoost
# 6.1. H√†m XGBoost Regressor
def xgb_regressor_analysis(df, ticker, test_size=0.3, random_state=42):
    from xgboost import XGBRegressor
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

    # 1. T√°ch features v√† target
    X = df.drop(columns='Closing Price')
    y = df['Closing Price']

    # 2. T√°ch t·∫≠p train/test theo th·ªùi gian
    split_index = int(len(df) * (1 - test_size))
    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

    # 3. Train XGBoost Regressor
    model = XGBRegressor(n_estimators=100, random_state=random_state)
    model.fit(X_train, y_train)

    # 4. D·ª± ƒëo√°n v√† ƒë√°nh gi√°
    y_pred = model.predict(X_test)

    print(f"ƒê√°nh gi√° m√¥ h√¨nh XGBoost Regressor - {ticker}:")
    print(f"MAE : {mean_absolute_error(y_test, y_pred):.4f}")
    print(f"MSE : {mean_squared_error(y_test, y_pred):.4f}")
    print(f"R¬≤  : {r2_score(y_test, y_pred):.4f}")

    # 5. Feature Importance
    importances = model.feature_importances_
    feature_names = X.columns
    feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)

    # 6. Bi·ªÉu ƒë·ªì
    plt.figure(figsize=(10, 6))
    sns.barplot(x=feat_imp.values, y=feat_imp.index)
    plt.title(f"Feature Importances (XGBoost Regressor) - {ticker}")
    plt.xlabel("Importance")
    plt.ylabel("Features")
    plt.tight_layout()
    plt.show()

# 7. H√†m XGBoost Classifier
def xgb_classifier_analysis(df, ticker, test_size=0.3, random_state=42):
    from xgboost import XGBClassifier
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

    # 1. T·∫°o nh√£n tƒÉng/gi·∫£m (1 = tƒÉng, 0 = gi·∫£m ho·∫∑c kh√¥ng ƒë·ªïi)
    df = df.copy()
    df['Target'] = (df['Closing Price'].shift(-1) > df['Closing Price']).astype(int)
    df.dropna(inplace=True)

    # 2. T√°ch features v√† target
    X = df.drop(columns=['Closing Price', 'Target'])
    y = df['Target']

    # 3. T√°ch t·∫≠p train/test theo th·ªùi gian
    split_index = int(len(df) * (1 - test_size))
    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

    # 4. Train XGBoost Classifier
    model = XGBClassifier(n_estimators=100, random_state=random_state, eval_metric='logloss')
    model.fit(X_train, y_train)

    # 5. D·ª± ƒëo√°n v√† ƒë√°nh gi√°
    y_pred = model.predict(X_test)

    print(f"ƒê√°nh gi√° m√¥ h√¨nh XGBoost Classifier - {ticker}:")
    print(f"Accuracy  : {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precision : {precision_score(y_test, y_pred):.4f}")
    print(f"Recall    : {recall_score(y_test, y_pred):.4f}")
    print(f"F1-score  : {f1_score(y_test, y_pred):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # 6. Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"Confusion Matrix - {ticker}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.show()

    # 7. Feature Importance
    importances = model.feature_importances_
    feature_names = X.columns
    feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(x=feat_imp.values, y=feat_imp.index)
    plt.title(f"Feature Importances (XGBoost Classifier) - {ticker}")
    plt.xlabel("Importance")
    plt.ylabel("Features")
    plt.tight_layout()
    plt.show()

# 8. Light GBM
# 8.1. Light GBM regressor
def lgbm_regressor_analysis(df, ticker, test_size=0.3, random_state=42):
    from lightgbm import LGBMRegressor
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

    # 1. T√°ch features v√† target
    X = df.drop(columns='Closing Price')
    y = df['Closing Price']

    # 2. T√°ch t·∫≠p train/test theo th·ªùi gian
    split_index = int(len(df) * (1 - test_size))
    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

    # 3. Train LightGBM Regressor
    model = LGBMRegressor(n_estimators=100, random_state=random_state)
    model.fit(X_train, y_train)

    # 4. D·ª± ƒëo√°n v√† ƒë√°nh gi√°
    y_pred = model.predict(X_test)

    print(f"ƒê√°nh gi√° m√¥ h√¨nh LightGBM Regressor - {ticker}:")
    print(f"MAE : {mean_absolute_error(y_test, y_pred):.4f}")
    print(f"MSE : {mean_squared_error(y_test, y_pred):.4f}")
    print(f"R¬≤  : {r2_score(y_test, y_pred):.4f}")

    # 5. Feature Importance
    importances = model.feature_importances_
    feature_names = X.columns
    feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)

    # 6. Bi·ªÉu ƒë·ªì
    plt.figure(figsize=(10, 6))
    sns.barplot(x=feat_imp.values, y=feat_imp.index)
    plt.title(f"Feature Importances (LightGBM Regressor) - {ticker}")
    plt.xlabel("Importance")
    plt.ylabel("Features")
    plt.tight_layout()
    plt.show()
# 8.2. Light GBM Classifier
def lgbm_classifier_analysis(df, ticker, test_size=0.3, random_state=42):
    from lightgbm import LGBMClassifier
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

    # 1. T·∫°o nh√£n tƒÉng/gi·∫£m (1 = tƒÉng, 0 = gi·∫£m ho·∫∑c kh√¥ng ƒë·ªïi)
    df = df.copy()
    df['Target'] = (df['Closing Price'].shift(-1) > df['Closing Price']).astype(int)
    df.dropna(inplace=True)

    # 2. T√°ch features v√† target
    X = df.drop(columns=['Closing Price', 'Target'])
    y = df['Target']

    # 3. T√°ch t·∫≠p train/test theo th·ªùi gian
    split_index = int(len(df) * (1 - test_size))
    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

    # 4. Train LightGBM Classifier
    model = LGBMClassifier(n_estimators=100, random_state=random_state)
    model.fit(X_train, y_train)

    # 5. D·ª± ƒëo√°n v√† ƒë√°nh gi√°
    y_pred = model.predict(X_test)

    print(f"ƒê√°nh gi√° m√¥ h√¨nh LightGBM Classifier - {ticker}:")
    print(f"Accuracy  : {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precision : {precision_score(y_test, y_pred):.4f}")
    print(f"Recall    : {recall_score(y_test, y_pred):.4f}")
    print(f"F1-score  : {f1_score(y_test, y_pred):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # 6. Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"Confusion Matrix - {ticker}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.show()

    # 7. Feature Importance
    importances = model.feature_importances_
    feature_names = X.columns
    feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(x=feat_imp.values, y=feat_imp.index)
    plt.title(f"Feature Importances (LightGBM Classifier) - {ticker}")
    plt.xlabel("Importance")
    plt.ylabel("Features")
    plt.tight_layout()
    plt.show()

# 9. Merge DataFrames
def merge_df(target, market, newfeat):
    target = pd.DataFrame(target)
    result = pd.merge(target, market, left_index=True, right_index=True, how='inner')
    result = pd.merge(result, newfeat, left_index=True, right_index=True, how='inner')
    result.dropna(inplace=True)
    return result